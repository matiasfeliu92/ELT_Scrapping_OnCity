# ğŸš€ Retail Competitive Intelligence: ELT Pipeline

Pipeline automatizado de **Web Scraping** y **Data Analytics** para el monitoreo de precios, stock y financiaciÃ³n en los principales retailers de Argentina: **Fravega, Megatone, Musimundo y Naldo**.

Este proyecto implementa una arquitectura **ELT** robusta, utilizando **Airflow** para la orquestaciÃ³n y **dbt** para la transformaciÃ³n siguiendo un modelo de **Arquitectura MedallÃ³n**.

---

## ğŸ›  Stack TecnolÃ³gico
* **OrquestaciÃ³n:** Apache Airflow
* **ExtracciÃ³n:** Python (Requests, Selenium)
* **Almacenamiento:** PostgreSQL & Docker
* **TransformaciÃ³n:** dbt (Data Build Tool)
* **CatÃ¡logo:** Google Sheets (Fuente original)

---

## ğŸ— Etapas del Proyecto

El flujo de datos sigue un ciclo de cuatro etapas bien definidas:

### 1. GestiÃ³n del CatÃ¡logo (Seeds)
El catÃ¡logo de productos se gestiona en un **Google Sheets**. Este se exporta como `product_catalog.csv` hacia la carpeta `seeds/` del proyecto dbt.
* **Comando:** `dbt seed`
* **Resultado:** CreaciÃ³n de la tabla de referencia en PostgreSQL con los links activos para scrapear.

### 2. ExtracciÃ³n (Scraping)
Utilizando **Selenium** y **Requests**, el sistema recorre cada uno de los links presentes en la tabla `product_catalog` cargada en la etapa anterior.
* **LÃ³gica:** Se prioriza la velocidad y el manejo de errores (logging) para asegurar que un link caÃ­do no detenga el pipeline.

### 3. Carga (Load)
Los datos extraÃ­dos se insertan directamente en la tabla `scraping_data` dentro del schema **`raw`** de PostgreSQL.
* **Formato:** Los datos se almacenan en formato **JSONB** para preservar la estructura original del scraping y permitir futuras auditorÃ­as.

### 4. TransformaciÃ³n (Arquitectura MedallÃ³n)
Una vez cargados los datos crudos, **dbt** toma el control para procesar la informaciÃ³n en capas:
* **Bronze:** Datos crudos con casting bÃ¡sico.
* **Silver:** Limpieza de strings, normalizaciÃ³n de precios y eliminaciÃ³n de duplicados.
* **Gold:** Modelos finales listos para BI (comparativas de precios, variaciones diarias, alertas de ofertas).



---

## ğŸ“‚ Estructura del Repositorio
```text
â”œâ”€â”€ products_scraping/ # Proyecto DBT
â”‚   â”œâ”€â”€ models/            # Capas Bronze, Silver, Gold
â”‚   â”œâ”€â”€ seeds/             # product_catalog.csv (CatÃ¡logo de productos)
â”‚   â””â”€â”€ dbt_project.yml
â”œâ”€â”€ dags/                  # OrquestaciÃ³n en Apache Airflow
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scripts/           # scraping.py, extract_data.py, load_data.py
â”‚   â”œâ”€â”€ config/            # ConfiguraciÃ³n de DB, Selectores y Logger
â”‚   â””â”€â”€ utils/             # extract_elements.py (Selenium helpers)
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml     # Infraestructura Postgres/Airflow
â””â”€â”€ main.py                # Entrada principal del proceso de extracciÃ³n