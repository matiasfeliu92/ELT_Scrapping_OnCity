# ğŸš€ Retail Competitive Intelligence: ELT Pipeline

Pipeline automatizado de **Web Scraping** y **Data Analytics** para el monitoreo de precios, stock y financiaciÃ³n en los principales retailers de Argentina: **Fravega, Megatone, Musimundo y Naldo**.

Este proyecto implementa una arquitectura **ELT** robusta, utilizando **Airflow** para la orquestaciÃ³n y **dbt** para la transformaciÃ³n siguiendo un modelo de **Arquitectura MedallÃ³n**.

---

## ğŸ“‘ MenÃº de NavegaciÃ³n
* [ğŸ›  Stack TecnolÃ³gico](#-stack-tecnolÃ³gico)
* [ğŸ— Arquitectura y Etapas](#-arquitectura-y-etapas)
* [ğŸ“‚ Estructura del Repositorio](#-estructura-del-repositorio)
* [ğŸ“‹ CatÃ¡logo de Productos](#-catÃ¡logo-de-productos)
* [âœ… Calidad de Datos y Logs](#-calidad-de-datos-y-logs)
* [ğŸš€ InstalaciÃ³n y Uso](#-instalaciÃ³n-y-uso)

---

## ğŸ›  Stack TecnolÃ³gico
* **OrquestaciÃ³n:** Apache Airflow
* **ExtracciÃ³n:** Python (Requests, Selenium)
* **Almacenamiento:** PostgreSQL & Docker
* **TransformaciÃ³n:** dbt (Data Build Tool)
* **CatÃ¡logo:** Google Sheets (Fuente original)

---

## ğŸ— Arquitectura y Etapas
El flujo de datos estÃ¡ diseÃ±ado para ser escalable y resiliente, dividiÃ©ndose en cuatro etapas clave:

1. **GestiÃ³n del CatÃ¡logo (dbt seeds):** Los productos se definen originalmente en un Google Sheets y se exportan a `products_scraping/seeds/product_catalog.csv`. Al ejecutar `dbt seed`, se crea la tabla de referencia en PostgreSQL que sirve como origen para el scraper.
2. **ExtracciÃ³n (Scraping):** El motor de scraping lee los links activos de la tabla cargada en el paso anterior y extrae informaciÃ³n mediante Selenium y Requests.
3. **Carga (Raw Data):** Los datos obtenidos se insertan en la tabla `scraping_data` (esquema `raw`) en formato **JSONB**, preservando la estructura original de cuotas y metadatos.
4. **TransformaciÃ³n (Medallion Architecture):** dbt procesa los datos en tres capas:
    * **Bronze:** Casting bÃ¡sico y limpieza inicial.
    * **Silver:** NormalizaciÃ³n de precios, eliminaciÃ³n de caracteres especiales y deduplicaciÃ³n.
    * **Gold:** Modelos finales listos para BI y anÃ¡lisis comparativo.



---

## ğŸ“‚ Estructura del Repositorio
```text
ELT_SCRAPPING_ONCITY/
â”œâ”€â”€ products_scraping/      # Proyecto dbt
â”‚   â”œâ”€â”€ models/             # Transformaciones (Bronze, Silver, Gold)
â”‚   â”œâ”€â”€ seeds/              # product_catalog.csv (Origen de la verdad)
â”‚   â””â”€â”€ dbt_project.yml
â”œâ”€â”€ dags/                   # OrquestaciÃ³n en Apache Airflow
â”œâ”€â”€ data/                   # Almacenamiento local de archivos JSON procesados
â”œâ”€â”€ logs/                   # Registros de ejecuciÃ³n de scripts y dbt
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/             # ConfiguraciÃ³n de DB, Logger y selectores
â”‚   â”œâ”€â”€ scripts/            # extract_data.py, load_data.py, scraping.py
â”‚   â””â”€â”€ utils/              # extract_elements.py, get_last_timestamp.py
â”œâ”€â”€ Dockerfile              # ConfiguraciÃ³n de imagen del entorno
â”œâ”€â”€ docker-compose.yml      # OrquestaciÃ³n de servicios
â”œâ”€â”€ main.py                 # Punto de entrada para el proceso de extracciÃ³n
â””â”€â”€ requirements.txt        # Dependencias de Python
```

---

## ğŸ“‹ CatÃ¡logo de Productos

El catÃ¡logo monitorea productos estratÃ©gicos con alta sensibilidad de precio:
* **LÃ­nea Blanca**: Heladeras (Samsung, Whirlpool, Electrolux) y Lavarropas (Drean, Samsung).
* **TecnologÃ­a**: Smart TVs (Samsung, Philips) y Smartphones (Motorola G56/G15, Galaxy A06).
* **PequeÃ±os ElectrodomÃ©sticos**: Microondas y Depiladoras.

---


## âœ… Calidad de Datos y Logs
* **Logging tÃ©cnico**: Captura fallos de selectores HTML y errores de navegaciÃ³n en extract_elements.py.
* **AuditorÃ­a de Carga**: Registro de registros procesados vs. fallidos en cada ciclo de scraping.
* **dbt Tests**: ValidaciÃ³n de integridad, unicidad y valores no nulos en las capas Silver y Gold.

---


## ğŸš€ InstalaciÃ³n y Uso
### 1. Clonar el repositorio de GitHub
```bash
git clone [https://github.com/matiasfeliu92/ELT_Scrapping_OnCity.git](https://github.com/matiasfeliu92/ELT_Scrapping_OnCity.git)
cd ELT_Scrapping_OnCity

2. **Acceder a la carpeta ELT_Scrapping_OnCity y crear entorno virtual de Python:**
PARA WINDOWS:
pip install virtualenv
virtualenv scraping
scraping/Scripts/activate

PARA LINUX:
python3 -m pip install virtualenv
virtualenv scraping
source scraping/bin/activate

3. **instalar modulos de Python:**
PARA WINDOWS:
pip install -r requirements.txt

PARA LINUX:
python3 -m pip install -r requirements.txt